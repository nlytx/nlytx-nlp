package cc.factorie.nlp.segment

import java.io.StringReader

import cc.factorie.nlp.{Document, DocumentAnnotator, Token}


/** Split a String into a sequence of Tokens.  Aims to adhere to tokenization rules used in Ontonotes and Penn Treebank.
  * Note that CoNLL tokenization would use tokenizeAllDashedWords=true.
  * Punctuation that ends a sentence should be placed alone in its own Token, hence this segmentation implicitly
  * defines sentence segmentation also. (Although our the DeterministicSentenceSegmenter does make a few adjustments
  * beyond this tokenizer.) This tokenizer can also normalize.
  **
  *This tokenizer is significantly faster than the DeterministicRegexTokenizer, using the EnglishLexer DFA
  *automatically generated by JFlex using the definition in EnglishLexer.flex.
  */

class DeterministicLexerTokenizer(
                                   val tokenizeSgml:Boolean = false, // Keep sgml/html tags as tokens
                                   val tokenizeNewline:Boolean = false, // Keep newlines as tokens
                                   val tokenizeWhitespace:Boolean = false, // Keep all whitespace, including newlines, as tokens
                                   val tokenizeAllDashedWords:Boolean = false, // Separate dashed words into separate tokens, such as in CoNLL
                                   val abbrevPrecedesLowercase:Boolean = false, // Assume a period followed by a lower case word is an abbrev and not end of sentence (see below)
                                   val normalize: Boolean = true, // Whether to normalize token strings
                                   val normalizeQuote:Boolean = true, // Convert all double quotes to "
                                   val normalizeApostrophe:Boolean = true, // Convert all apostrophes to ', even within token strings
                                   val normalizeCurrency:Boolean = true, // Convert all currency symbols to "$", except cents symbol to "cents"
                                   val normalizeAmpersand:Boolean = true, // Convert all ampersand symbols (including "&amp;" to "&"
                                   val normalizeFractions:Boolean = true, // Convert unicode fraction characters to their spelled out analogues, like "3/4"
                                   val normalizeEllipsis:Boolean = true, // Convert unicode ellipsis character to spelled out analogue, "..."
                                   val undoPennParens:Boolean = true, // Change -LRB- etc to "(" etc.
                                   val unescapeSlash:Boolean = true, // Change \/ to /
                                   val unescapeAsterisk:Boolean = true, // Change \* to *
                                   val normalizeMDash:Boolean = true, // Convert all em-dashes to double dash --
                                   val normalizeDash:Boolean = true, // Convert all other dashes to single dash -
                                   val normalizeHtmlSymbol:Boolean = true, // Convert &lt; to <, etc
                                   val normalizeHtmlAccent:Boolean = true // Convert Beyonc&eacute; to Beyonce
                                 ) extends DocumentAnnotator {

  /** How the annotation of this DocumentAnnotator should be printed in one-word-per-line (OWPL) format.
      If there is no per-token annotation, return null.  Used in Document.owplString. */
  def tokenAnnotationString(token: Token) = token.stringStart.toString+'\t'+token.stringEnd.toString

  val lexer =
  // here we make sure that if normalize = false, we really don't normalize anything
    if(normalize)
      new EnglishLexer(null, tokenizeSgml, tokenizeNewline, tokenizeWhitespace, tokenizeAllDashedWords, abbrevPrecedesLowercase,
        normalizeQuote, normalizeApostrophe, normalizeCurrency, normalizeAmpersand, normalizeFractions, normalizeEllipsis,
        undoPennParens, unescapeSlash, unescapeAsterisk, normalizeMDash, normalizeDash, normalizeHtmlSymbol, normalizeHtmlAccent)
    else
      new EnglishLexer(null, tokenizeSgml, tokenizeNewline, tokenizeWhitespace, tokenizeAllDashedWords, abbrevPrecedesLowercase,
        false, false, false, false, false, false, false, false, false, false, false, false, false)

  def process(document: Document): Document = {
    for (section <- document.sections) {
      /* Add this newline to avoid JFlex issue where we can't match EOF with lookahead */
      val reader = new StringReader(section.string + "\n")
      lexer.yyreset(reader)

      var currentToken = lexer.yylex().asInstanceOf[(String, Int, Int)]
      while (currentToken != null){
        if (abbrevPrecedesLowercase && section.length > 1 && section.tokens.last.string == "." && java.lang.Character.isLowerCase(currentToken._1(0)) && section.tokens(section.length-2).stringEnd == section.tokens(section.length-1).stringStart) {
          // If we have a pattern like "Abbrev. has" (where "has" is any lowercase word) with token strings "Abbrev", ".", "is" (currently looking at "is")
          // then assume that the previous-previous word is actually an abbreviation; patch it up to become "Abbrev.", "has".
          val lastTwoTokens = section.takeRight(2).toIndexedSeq
          section.remove(section.length - 1); section.remove(section.length - 1)
          new Token(section, lastTwoTokens(0).stringStart, lastTwoTokens(1).stringEnd)
        }
        val tok = new Token(section, currentToken._2, currentToken._2 + currentToken._3)
        if(normalize && tok.string != currentToken._1) tok.attr += new PlainNormalizedTokenString(tok, currentToken._1)
        currentToken = lexer.yylex().asInstanceOf[(String, Int, Int)]
      }
      /* If tokenizing newlines, remove the trailing newline we added */
      if(tokenizeNewline) section.remove(section.tokens.length - 1)
    }
    if (!document.annotators.contains(classOf[Token]))
      document.annotators(classOf[Token]) = this.getClass
    document
  }

  def prereqAttrs: Iterable[Class[_]] = Nil
  def postAttrs: Iterable[Class[_]] = List(classOf[Token])

  /** Convenience function to run the tokenizer on an arbitrary String.  The implementation builds a Document internally, then maps to token strings. */
  def apply(s:String): Seq[String] = process(new Document(s)).tokens.toSeq.map(_.string)
}